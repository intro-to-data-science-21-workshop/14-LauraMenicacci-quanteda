---
title: "Quanteda Package"
subtitle: "Sentiment analysis using the Manifesto Corpus" #maybe "Quantitative Text analysis in R using the Manifesto Corpus"
author: "Laura Menicacci, Dinah Rabe"
date: "04/11/2021"
output: 
  html_document:
    toc: true
    toc_collapsed: true
    toc_float: true
    toc_depth: 3
    number_sections: false
    theme: lumen
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Welcome to our tutorial!

In the following minutes we will go through the basics of the **Quanteda** package and make sense of what quantitative text analysis is in practice, with some (hopefully) interesting examples. 

In this session you will learn to:

* load and work with text databases
* create the basic elements for a proper QTA, namely Corpus, Tokens, and a Document-Feature Matrix
* proceed with some statistical analyses of texts
* try out some unsupervised learning bites, in particular sentiment analysis

As already mentioned in the slides, we will make use of the Manifesto Project Database (see more on https://manifesto-project.wzb.eu/). 

**Let's start!**

##Necessary packages

Here you find the main packages you'll need to install, plus some recommended ones that can help to extend or complete some functionalities (e.g. readtext takes files and returns them in a type of data.frame that can be used directly with the corpus() constructor function).

```{r}
#install.packages("tidyverse")
#install.packages("quanteda")
#install.packages("quanteda.textstats")
## if you get an error for this, try: install.packages("quanteda.textstats", type="binary")
```


```{r}

library(tidyverse)
library(quanteda)

# library(quanteda.textmodels) 
# library(quanteda.textplots)
library(quanteda.textstats)

library(readtext)

```

For our tutorial we don't need the other extension packages (.textmodels & .textplots), but maybe you need them for future use cases, so we included them in the code snippet.

# Data Importing

Since we are using the online database of the Manifesto Project for our tutorial, the way we import the data is a bit *special* (using an API-key). Normally, you maybe want to import a pre-formatted file that you have stored on your computer, or multiple files that are stored in a folder. In the following, you find some explanations how to import these kinds of data. 

As examples we will use the included data of the quanteda package. It is data about the Inaugural Speeches of American Presidents (you will find this data in almost all tutorials that are online).To import data we use the **readtext** package. 

## Pre-formatted files

Pre-formatted files are mainly imported in a “spreadsheet format”. **path_data** is the location of sample files that we will use.

```{r}
path_data <- system.file("extdata/", package = "readtext")
```

Most of the times, the pre-formatted file is stored with one column containing the text and additional columns storing document-level variables. If this is the case, then we can use read.csv() to import.

```{r}
dat_inaug <- read.csv(paste0(path_data, "/csv/inaugCorpus.csv"))
```

Alternatively, it is possible to import character values (comma- or tab-separated). readtext is able to import and read any file containing text and any associated document-level variable, as in this example. 

```{r pressure, echo=FALSE}
dat_dail <- readtext(paste0(path_data, "/tsv/dailsample.tsv"), text_field = "speech")
```

## Multiple text files

We can also load multiple text files at once that are stored in the same folder or subfolders. Individual text files usually do not contain document-level variables, but we can create them.

In this example, the directory /txt/UDHR contains text files (".txt”) of the Universal Declaration of Human Rights in 13 languages.

You can generate document-level variables based on the file names using the **docvarnames** and **docvars** from argument. 

- **dvsep = "_"** specifies the value separator in the filenames. 
- **encoding = "ISO-8859-1"** determines character encodings of the texts.

```{r}
dat_udhr <- readtext(paste0(path_data, "/txt/UDHR/*"))

dat_eu <- readtext(paste0(path_data, "/txt/EU_manifestos/*.txt"),
                    docvarsfrom = "filenames", 
                    docvarnames = c("unit", "context", "year", "language", "party"),
                    dvsep = "_", 
                    encoding = "ISO-8859-1")
str(dat_eu)

#If you are using Windows, you need might need to specify the encoding of the file by adding encoding = "utf-8". In this case, imported texts might appear like <U+4E16><U+754C><U+4EBA><U+6743> but they indicate that Unicode characters are imported correctly.
```

These are just some of the multiple ways to load data with quanteda. For furher information (if you may want to know how to upload text with different encodings) take a look at https://tutorials.quanteda.io/import-data/. 


# Working with ManifestoR - Corpus

To load the Manifesto Corpus, which is a subclass of a `Corpus` object - the one we need to perform a text analysis - we need to load a specific package ManifestoR. 

With the function `mp_setapikey` we can easily load the API key that is needed in order to access the website. We then use the `mp_corpus` function to access documents in the ManifestoCorpus that we can directly use for our analysis.  

As we know from the presentation, a Corpus consists of many documents. In the `ManifestoCorpus`, documents can be indexed via their `manifesto_id`, which is an identification code formed by the CMP party code, an underscore, and either the election year, if unambiguous, or the election year and month or via their position in the corpus. For further information check their amazing tutorial on [using the Manifesto Corpus with quanteda](https://manifesto-project.wzb.eu/down/tutorials/quanteda)

## Our subset for this tutorial
We will use data from the election programmes in the United Kingdom to perform some basics of quantitative text analysis and a sentiment analysis. 

We use the quanteda `corpus()` to create the Corpus object from available sources, letting it auto-generate document names based on the manifesto_id and a within document running number, as we can see in `corpus(docid_field = "manifesto_id", unique_docnames = FALSE)`. 

```{r}
# install.packages("manifestoR")
library(manifestoR)

mp_setapikey(key.file = "manifesto_apikey.txt")

uk_election_programmes <- mp_corpus(countryname == "United Kingdom") 

uk_election_programmes
names(uk_election_programmes) #do we want to have that in the html as output?

manifesto_corpus <- uk_election_programmes %>%
  as.data.frame(with.meta = TRUE) %>%
  corpus(docid_field = "manifesto_id", unique_docnames = FALSE) %>% 
  #docvars(field = "cmp_code") %>% 
  corpus() 

manifesto_corpus

manifesto_corpus %>%
  docvars() %>% 
  names() #meta data information


```

## Subsetting the Corpus

We will do the following analyses using only one party in the English parliament. We will take the Scottish National Party, subsetting the corpus with `corpus_subset()` and indicating a logical condition corresponding to the code of the party. 

METTI LINK PER TROVARE I CODICI DEI PARTIES # i guess that means that we need to inlucde a link where people find the parties codes? ;)


```{r}
scotland_corpus <- manifesto_corpus %>%
  corpus_subset(party == "51902")

```

## Tokenization

Tokenization is particularly important for pre-processing and cleaning the texts. It is possible to easily remove numbers, punctuation or stopwords. Moreover, it is simple to transform all text to lower case or stem words. 

```{r}
tokenized_manifesto <- scotland_corpus %>%
  tokens() %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>% 
  tokens_remove(c("will")) %>% #remove will 
  tokens_wordstem()

tokenized_manifesto
```

## Document-Feature-Matrix

We are now able to construct our document-feature matrix. 

The ManifestoCorpus requires also some stardand preprocessing and specific subsetting regarding cmp codes and the quasi-sentences structure. 

In this code snippet we follow this process:

* drop all quasi-sentences with headline codes (“H”), uncoded (“0”,“000”) and with codes missing (NA). 
* group all quasi-sentences coded with the same code to one document
* transform term frequencies using the `dfm_weight()` function that to count the proportion of words per document `(scheme = "prop")`.
* subset the dfm features for three specific codes: **European Union: Positive** (108), **European Union: Negative** (110) and **Federalism** (301). 

```{r}
manifesto_dfm <- tokenized_manifesto %>% 
  dfm() %>% 
  dfm_subset(!(cmp_code %in% c("H", "", "0", "000", NA))) %>% 
  dfm_group(cmp_code) %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_subset(cmp_code %in% c("301", "110", "108")) 

manifesto_dfm
```


# Plotting frequencies

Going through a classical quantitative text analysis, you may want to plot the most frequent terms, both for exploratory and explanatory purposes. Then, the `textstat_frequency()` function fits perfectly for you. It simply extracts the most frequent terms (here grouped by the cmp_code we chose precedently) and converts them to a data frame that here is plotted with the ggplot package. 
.
```{r}
feature_frequencies_cat <- manifesto_dfm %>% textstat_frequency(n = 10, group = cmp_code)

feature_frequencies_cat %>%
  mutate(cmp_code = factor(group, labels = c("European Union: Positive", "European Union: Negative", "Federalism"))) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = cmp_code)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "share of words per category") +
  facet_wrap(~cmp_code, ncol = 2, scales = "free") +
  coord_flip()

```

## Keyword in context search 

Quanteda also provides an effective method for reading text passages based on  key words. `kwic()` ("keyword in context") allows to do it for a text string or pattern.

Specifically here we choose the keyword "brexit", and we select the 10 words around (before and after) that specific word. 

```{r}

scotland_corpus %>%
  tokens() %>%
  kwic(phrase("brexit"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))


```

# Targeted sentiment analysis

Quanteda is also useful if we want to perform sentiment analysis. In the following example we perform targeted sentiment analysis. We use the whole manifesto corpus loaded at the beginning and tokenize it into words. We then keep only tokens that include the word “Brexit” as well as the ten words before and after every occurrence of it. 

We use the sentiment dictionary constructed by Young & Soroka (2012) that is integrated in the package, stored in `data_dictionary_LSD2015`. It contains thousands of positive and negative words or word stems.

```{r}
brexit_tokens <- manifesto_corpus %>%
  tokens() %>% 
  tokens_select("brexit", selection = "keep", window = 20, padding = FALSE, verbose = TRUE)

brexit_sent <- brexit_tokens %>%
  dfm() %>% 
  dfm_lookup(data_dictionary_LSD2015[1:2]) %>% 
  dfm_group(party)

print(brexit_sent)
  

```

# Plotting sentiment analysis

```{r}
sent_df <- convert(brexit_sent, to = "data.frame")

sent_df$sent_score <- log((sent_df$positive + 0.5) / (sent_df$negative + 0.5))

sent_df %>% 
  pivot_longer(c(positive, negative)) %>% 
  ggplot(aes(x = reorder(doc_id, sent_score), y = sent_score), fill =  as.factor(name)) + 
  geom_point(size = 3) +
  coord_flip() + 
  labs(x = "Parties", y = "Estimated sentiment")
 

###TO DO
#CHANGE NAME OF PARTIES
#change numbers on x axis
#subset excel file 
#change names of parties

```


# CREDITS 

https://tutorials.quanteda.io

https://github.com/quanteda

Burst, Tobias / Krause, Werner / Lehmann, Pola / Lewandowski, Jirka / Matthieß, Theres / Merz, Nicolas / Regel, Sven / Zehnter, Lisa (2021): Manifesto Corpus. Version: 2021-1. Berlin: WZB Berlin Social Science Center.

Pennings, Paul / Keman, Hans, Vrije Universiteit Amsterdam, Comparative Electronic Manifestos Project in cooperation with the Social Science Research Centre Berlin (Andrea Volkens, Hans-Dieter Klingemann), the Zentralarchiv für empirische Sozialforschung (GESIS), and the Manifesto Research Group (chairman: Ian Budge) (2006)

Volkens, Andrea / Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2021): The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR). Version 2021a. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpds.2021a

Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Merz, Nicolas / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2020): The Manifesto Data Collection: South America. Version 2020b. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpdssa.2020b
---
title: "Quanteda Package"
subtitle: "Sentiment analysis using the Manifesto Corpus"
author: "Laura Menicacci, Dinah Rabe"
date: "04/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to our tutorial!

In the following minutes we will go through the basics of the **Quanteda** package and make sense of what quantitative text analysis is in practice, with some (hopefully) interesting examples. 

In this session you will learn to: 
* load and work with text databases
* create the basic elements for a proper QTA, namely Corpus, Tokens, and a Document-Feature Matrix
* proceed with some statistical analyses of texts
* try out some unsupervised learning bites, in particular sentiment analysis

As already mentioned in the slides, we will make use of the Manifesto Project Database (see more on https://manifesto-project.wzb.eu/). 

Let's start!

##Necessary packages

Here you find the main packages you'll need to install, plus some recommended ones that can help to extend or complete some functionalities (e.g. readtext takes files and returns them in a type of data.frame that can be used directly with the corpus() constructor function).

```{r}
install.packages("tidyverse", "quanteda")
```


```{r}

library(tidyverse)
library(quanteda)

library(quanteda.textmodels) 
library(quanteda.textplots)
library(quanteda.textstats)

library(readtext)

```

# Data Importing

We uploaded on Github the csv version of the Manifesto Project Dataset (latest version), so that you can easily download it and upload it using the following commands. 

To import data we use the **readtext** package. 

## Pre-formatted files

Pre-formatted files are mainly imported in a “spreadsheet format”. **path_data** is the location of sample files that we will use.

```{r}
path_data <- system.file("extdata/", package = "readtext")
```

Most of the times, the pre-formatted file is stored with one column containing the text and additional columns storing document-level variables. If this is the case, then we can use read.csv() to import.

```{r}
dat_inaug <- read.csv(paste0(path_data, "/csv/inaugCorpus.csv"))
```

Alternatively, it is possible to import character values (comma- or tab-separated). readtext is able to import and read any file containing text and any associated document-level variable, as in this example. 

```{r pressure, echo=FALSE}
dat_dail <- readtext(paste0(path_data, "/tsv/dailsample.tsv"), text_field = "speech")
```

# Multiple text files

We can also load multiple text files at once that are stored in the same folder or subfolders. Individual text files usually do not contain document-level variables, but we can create them.

In this example, the directory /txt/UDHR contains text files (".txt”) of the Universal Declaration of Human Rights in 13 languages.

You can generate document-level variables based on the file names using the **docvarnames** and **docvars** from argument. 

- **dvsep = "_"** specifies the value separator in the filenames. 
- **encoding = "ISO-8859-1"** determines character encodings of the texts.

```{r}
dat_udhr <- readtext(paste0(path_data, "/txt/UDHR/*"))

dat_eu <- readtext(paste0(path_data, "/txt/EU_manifestos/*.txt"),
                    docvarsfrom = "filenames", 
                    docvarnames = c("unit", "context", "year", "language", "party"),
                    dvsep = "_", 
                    encoding = "ISO-8859-1")
str(dat_eu)

#If you are using Windows, you need might need to specify the encoding of the file by adding encoding = "utf-8". In this case, imported texts might appear like <U+4E16><U+754C><U+4EBA><U+6743> but they indicate that Unicode characters are imported correctly.
```

These are just some of the multiple ways to load data with quanteda. For furher information (if you may want to know how to upload text with different encodings) take a look at https://tutorials.quanteda.io/import-data/. 


#Working with ManifestoR - Corpus

To load the Manifesto Corpus, which is a subclass of a `Corpus` object - the one we need to perform a text analysis - we need to load a specific package ManifestoR. 

With the function `mp_setapikey` we can easily load the API key that is needed in order to access the website. We then use the `mp_corpus` function to access documents in the ManifestoCorpus that we can directly use for our analysis.  

As we know from the presentation, a Corpus consists of many documents. In the `ManifestoCorpus`, documents can be indexed via their `manifesto_id`, which is an identification code formed by the CMP party code, an underscore, and either the election year, if unambigous, or the election year and month) or via their position in the corpus. 

We will use data from the election programmes in the United Kingdom to perform basic QTA and a sentiment analysis. 

We use the quanteda `corpus()` to create the Corpus object from available sources, letting it auto-generate document names based on the manifesto_id and a within document running number, as we can see in `corpus(docid_field = "manifesto_id", unique_docnames = FALSE)`. 

```{r}
library(manifestoR)

mp_setapikey(key.file = "manifesto_apikey.txt")

uk_election_programmes <- mp_corpus(countryname == "United Kingdom") 

uk_election_programmes
names(uk_election_programmes)

manifesto_corpus <- uk_election_programmes %>%
  as.data.frame(with.meta = TRUE) %>%
  corpus(docid_field = "manifesto_id", unique_docnames = FALSE) %>% 
  #docvars(field = "cmp_code") %>% 
  corpus() 

manifesto_corpus

manifesto_corpus %>%
  docvars() %>% 
  names() #meta data information


```

#Subsetting the Corpus

We will do the following analyses using only one party in the English parliament. We will take the Scottish National Party, subsetting the corpus with `corpus_subset()` and indicating a logical condition corresponding to the code of the party. 

METTI LINK PER TROVARE I CODICI DEI PARTIES


```{r}
scotland_corpus <- manifesto_corpus %>%
  corpus_subset(party == "51902")

```

#Tokenization

Tokenization is particularly important for pre-processing and cleaning the texts. It is possible to easily remove numbers, punctuation or stopwords. Moreover, it is simple to transform all text to lower case or stem words. 

```{r}
tokenized_manifesto <- scotland_corpus %>%
  tokens() %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>% ##remove will 
  tokens_wordstem()

tokenized_manifesto
```

#Document-Feature-Matrix

We are now able to construct our document-feature matrix. 

The ManifestoCorpus requires also some specific polishing regarding cmp codes and the quasi-sentences structuring. 

In this code snippet we follow this process: 
- drop all quasi-sentences with headline codes (“H”), uncoded (“0”,“000”) and with codes missing (NA). 
- group all quasi-sentences coded with the same code to one document
- transform term frequencies using the `dfm_weight()` function that we use to calculate the proportion of words per document `(scheme = "prop")`.
- subset the dfm features for three specific codes: **European Union: Positive** (108), **European Union: Negative** (110) and **Federalism** (301). 

```{r}
manifesto_dfm <- tokenized_manifesto %>% 
  dfm() %>% 
  dfm_subset(!(cmp_code %in% c("H", "", "0", "000", NA))) %>% 
  dfm_group(cmp_code) %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_subset(cmp_code %in% c("301", "110", "108")) 

manifesto_dfm
```


#Plotting frequencies

If you may want to plot the most frequent terms, then the textstat_frequency() function fits perfectly for you. It simply extracts the most frequent terms (here grouped by cmp_code) and converts them to a data.frame that can be plotted with the ggplot package
.
```{r}
feature_frequencies_cat <- manifesto_dfm %>% textstat_frequency(n = 10, group = cmp_code)

feature_frequencies_cat %>%
  mutate(cmp_code = factor(group, labels = c("European Union: Positive", "Federalism", "European Union: Negative"))) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = cmp_code)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "share of words per category") +
  facet_wrap(~cmp_code, ncol = 2, scales = "free_y") +
  coord_flip()


##add counts

```

#Keyword in context search 

Quanteda also provides a nice way to view text passages based on certain key words. The kwic (for keyword in context) allows you to use for a text string or pattern.

```{r}

scotland_corpus %>%
  tokens() %>%
  kwic(phrase("brexit"), window = 10) %>%
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))


```

#Targeted sentiment analysis

Quanteda facilitates dictionary based searchs. The following example illustrates how to conduct a targeted sentiment analysis. We use the corpus created above based on Scotland party and tokenize it into words. We then keep only tokens that include the word “President” as well as the ten words before and after every occurence of “Brexit”.

Quanteda has integrated a sentiment dictionary constructed by Young & Soroka (2012) stored in data_dictionary_LSD2015. The dictionary contains thousands of positive and negative words or word stems.


```{r}
brexit_tokens <- manifesto_corpus %>%
  #corpus_subset(party %in% c("513202", "51420")) %>%  #labour and liberal 
  tokens() %>% 
  tokens_select("Brexit", selection = "keep", window = 10, padding = FALSE, verbose = TRUE)

brexit_sent <- brexit_tokens %>%
  dfm() %>% 
  dfm_lookup(data_dictionary_LSD2015[1:2]) %>% 
  dfm_group(party)

print(brexit_sent)
  

```

---> PLOT SENTIMENT ANALYSIS

Quanteda (and its “sister”-packages quanteda.textstats, quanteda.textplots, quanteda.textmodels) has many more functions. In particular the textstat_* functions of quanteda.textstats are powerful and can well applied to manifestos.



```{r}
dict_output <- convert(brexit_sent, to = "data.frame")

dict_output$sent_score <- log((dict_output$positive + 0.5) / (dict_output$negative + 0.5))
dict_output <- cbind(dict_output, docvars(manifesto_corpus))

tplot_sent <- ggplot(dict_output, aes(x =doc_id , y = reorder(sent_score)) + geom_point(size = 3) +
coord_flip() + labs(x = "Parties", y = "Estimated sentiment"))

tplot_sent
```


###CREDITS 

https://tutorials.quanteda.io
https://github.com/quanteda

Burst, Tobias / Krause, Werner / Lehmann, Pola / Lewandowski, Jirka / Matthieß, Theres / Merz, Nicolas / Regel, Sven / Zehnter, Lisa (2021): Manifesto Corpus. Version: 2021-1. Berlin: WZB Berlin Social Science Center.

Pennings, Paul / Keman, Hans, Vrije Universiteit Amsterdam, Comparative Electronic Manifestos Project in cooperation with the Social Science Research Centre Berlin (Andrea Volkens, Hans-Dieter Klingemann), the Zentralarchiv für empirische Sozialforschung (GESIS), and the Manifesto Research Group (chairman: Ian Budge) (2006)

Volkens, Andrea / Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2021): The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR). Version 2021a. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpds.2021a

Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Merz, Nicolas / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2020): The Manifesto Data Collection: South America. Version 2020b. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpdssa.2020b
---
title: "Quanteda Package"
subtitle: "Quantitative Text analysis in R using the Manifesto Corpus"
author: "Laura Menicacci, Dinah Rabe"
date: "04/11/2021"
output: 
  html_document:
    toc: true
    toc_collapsed: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    number_sections: false
    theme: lumen
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Welcome to our tutorial! `r emo::ji("books")`

In the following minutes we will go through the basics of the **Quanteda** package and make sense of what quantitative text analysis is in practice, with some (hopefully) interesting examples. 

In this session you will learn to:

* load and work with text databases
* create the basic elements for a proper QTA, namely Corpus, Tokens, and a Document-Feature Matrix
* proceed with some exploratory analysis of texts
* try out some sentiment analysis of UK electoral programmes

As already mentioned in the slides, we will make use of the Manifesto Project Database (see more on https://manifesto-project.wzb.eu/). 

**Let's start!** `r emo::ji("biceps")`

## Necessary packages

Here you find the main packages you'll need to install, plus some recommended ones that can help to extend or complete some functionalities (e.g. readtext takes files and returns them in a type of data.frame that can be used directly with the corpus() constructor function).

```{r}
#install.packages("tidyverse")
#install.packages("quanteda")
#install.packages("quanteda.textstats")
## if you get an error for this, try: install.packages("quanteda.textstats", type="binary")
```


```{r}
library(tidyverse)
library(quanteda)

# library(quanteda.textmodels) 
# library(quanteda.textplots)
library(quanteda.textstats)

library(readtext)
```

For our tutorial we don't need the other extension packages (.textmodels & .textplots), but maybe you need them for future use cases, so we included them in the code snippet.

# Data Importing `r emo::ji("computer")`

Since we are using the online database of the Manifesto Project for our tutorial, the way we import the data is a bit *special* (using an API-key). Normally, you maybe want to import a pre-formatted file that you have stored on your computer, or multiple files that are stored in a folder. In the following, you find some explanations how to import these kinds of data. 

As examples we will use the included data of the quanteda package. It is data about the Inaugural Speeches of American Presidents (you will find this data in almost all tutorials that are online). To import data we use the **readtext** package. 

## Pre-formatted files

Pre-formatted files are mainly imported in a “spreadsheet format”. **path_data** is the location of sample files that we will use.

```{r}
path_data <- system.file("extdata/", package = "readtext")
```

Most of the times, the pre-formatted file is stored with one column containing the text and additional columns storing document-level variables. If this is the case, then we can use read.csv() to import.

```{r}
dat_inaug <- read.csv(paste0(path_data, "/csv/inaugCorpus.csv"))
```

Alternatively, it is possible to import character values (comma- or tab-separated). readtext is able to import and read any file containing text and any associated document-level variable, as in this example. 

```{r pressure, echo=FALSE}
dat_dail <- readtext(paste0(path_data, "/tsv/dailsample.tsv"), text_field = "speech")
```

## Multiple text files

We can also load multiple text files at once that are stored in the same folder or subfolders. Individual text files usually do not contain document-level variables, but we can create them.

In this example, the directory "/txt/UDHR" contains text files (".txt”) of the Universal Declaration of Human Rights in 13 languages.

You can generate document-level variables based on the file names using the **docvarnames** and **docvars** from argument. 

- dvsep = "_" specifies the value separator in the filenames. 
- encoding = "ISO-8859-1" determines character encodings of the texts.
```{r}
dat_udhr <- readtext(paste0(path_data, "/txt/UDHR/*"))

dat_eu <- readtext(paste0(path_data, "/txt/EU_manifestos/*.txt"),
                    docvarsfrom = "filenames", 
                    docvarnames = c("unit", "context", "year", "language", "party"),
                    dvsep = "_", 
                    encoding = "ISO-8859-1")
str(dat_eu)

#If you are using Windows, you might need to specify the encoding of the file by adding encoding = "utf-8". In this case, imported texts might appear like <U+4E16><U+754C><U+4EBA><U+6743> but they indicate that Unicode characters are imported correctly.
```

These are just some of the multiple ways to load data with quanteda. For furher information (if you may want to know how to upload text with different encodings) take a look at https://tutorials.quanteda.io/import-data/. 


# Working with ManifestoR `r emo::ji("paper")`

To load the Manifesto Corpus, which is a subclass of a Corpus object - the one we need to perform a text analysis - we need to load a specific package: ManifestoR. 

With the function `mp_setapikey` we can easily load the API key that is needed in order to access the website. We then use the `mp_corpus` function to access documents in the ManifestoCorpus that we can directly use for our analysis.  

As we know from the presentation, a Corpus consists of many documents. In the `ManifestoCorpus`, documents can be indexed via their `manifesto_id`, which is an identification code formed by the CMP party code, an underscore, and either the election year, if unambiguous, or the election year and month or via their position in the corpus. For further information check their amazing tutorial on [using the Manifesto Corpus with quanteda](https://manifesto-project.wzb.eu/down/tutorials/quanteda). 

## Our subsetted Corpus for this tutorial
We will use data from the electoral programmes in the United Kingdom to perform some basics of quantitative text analysis and a sentiment analysis. With the function `names(uk_electoral_programmes)`we can visualize how documents are named. The names indeed consist of the code of the parties on the left of underscore, and the date of the documents on the right side of the underscore.

We use the quanteda `corpus()` to create the Corpus object from available sources, letting it auto-generate document names based on the manifesto_id and a within document running number, as we can see in `corpus(docid_field = "manifesto_id", unique_docnames = FALSE)`. 

It is also useful to understand how document-level variables are structured with `docvars()`, since they represent metadata information. 
```{r}
# install.packages("manifestoR")
library(manifestoR)

mp_setapikey(key.file = "manifesto_apikey.txt")

uk_electoral_programmes <- mp_corpus(countryname == "United Kingdom") 
uk_electoral_programmes

manifesto_corpus <- uk_electoral_programmes %>%
  as.data.frame(with.meta = TRUE) %>%
  corpus(docid_field = "manifesto_id", unique_docnames = FALSE) %>% 
  corpus() 
manifesto_corpus

manifesto_corpus %>%
  docvars() %>% 
  names()
```

## Subsetting the Corpus

We will do the following analyses using only one party in the English parliament. We will take the Scottish National Party, subsetting the corpus with `corpus_subset()` and indicating a logical condition corresponding to the code of the party. 

To check to which code the Scottish National Party corresponds, we looked at the MPDataset, containing all the information on the electoral programs, and we quickly subsetted it for our purposes. 
```{r}
library(readr)
MPDataset <- read_csv("MPDataset_MPDS2021a.csv")

MPD_uk_parties <- MPDataset %>% 
  subset(countryname == "United Kingdom") %>% 
  select(party, partyname) %>% 
  distinct()

scotland_corpus <- manifesto_corpus %>%
  corpus_subset(party == "51902")
```

## Tokenization

Tokenization is particularly important for pre-processing and cleaning the texts. It is possible to easily remove numbers, punctuation or stopwords. Moreover, it is simple to transform all text to lower case or stem words. 
```{r}
tokenized_manifesto <- scotland_corpus %>%
  tokens() %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>% 
  tokens_remove(c("will")) %>% #remove will 
  tokens_wordstem()

tokenized_manifesto
```

## Document-Feature-Matrix

We are now able to construct our document-feature matrix. 

The ManifestoCorpus requires some stardand preprocessing, plus since we want to analyse specific topics regarding the Scottish National Party, we subset specific cmp codes and the documents structure. 

In this code snippet we follow this process:

* drop all quasi-sentences with headline codes (“H”), uncoded (“0”,“000”) and with codes missing (NA). 
* group all quasi-sentences coded with the same code to one document
* transform term frequencies using the `dfm_weight()` function that to count the proportion of words per document `(scheme = "prop")`
* subset the dfm features for three specific codes: **European Union: Positive** (108), **European Union: Negative** (110) and **Federalism** (301). 
```{r}
manifesto_dfm <- tokenized_manifesto %>% 
  dfm() %>% 
  dfm_subset(!(cmp_code %in% c("H", "", "0", "000", NA))) %>% 
  dfm_group(cmp_code) %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_subset(cmp_code %in% c("301", "110", "108")) 

manifesto_dfm
```

# Some explorative analysis `r emo::ji("search")`

## Plotting frequencies

Going through a classical quantitative text analysis, you may want to plot the most frequent terms, especially for exploratory purposes. Then the `textstat_frequency()` function fits perfectly for you. It simply extracts the most frequent terms (here grouped by the cmp codes we chose precedently) and converts them to a data frame that here is plotted with the ggplot package. 
```{r}
feature_frequencies_cat <- manifesto_dfm %>% textstat_frequency(n = 10, group = cmp_code)

feature_frequencies_cat %>%
  mutate(cmp_code = factor(group, labels = c("European Union: Positive", "European Union: Negative", "Federalism"))) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = cmp_code)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "share of words per category") +
  facet_wrap(~cmp_code, ncol = 2, scales = "free") +
  coord_flip()
```

## Keyword in context search 

Quanteda also provides an effective method for reading text passages based on key words. `kwic()` ("keyword in context") allows to do it for a text string or pattern. 
Specifically here we choose the keyword "Brexit", and we select the 10 words around (before and after) that specific word. 
```{r}
scotland_corpus %>%
  tokens() %>%
  kwic(phrase("brexit"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
```
 
# Targeted sentiment analysis `r emo::ji("angel")` `r emo::ji("devil")`

Quanteda is also useful if we want to perform sentiment analysis. In the following example we perform targeted sentiment analysis. We use the whole manifesto corpus loaded at the beginning and tokenize it into words. We then keep only tokens that include the word “Brexit” as well as the ten words before and after every occurrence of it. 

We use the sentiment dictionary constructed by Young & Soroka (2012) that is integrated in the package, stored in `data_dictionary_LSD2015`. It contains thousands of positive and negative words or word stems.
```{r}
brexit_tokens <- manifesto_corpus %>%
  tokens() %>% 
  tokens_select("Brexit", selection = "keep", window = 20, padding = FALSE, verbose = TRUE)

brexit_sent <- brexit_tokens %>%
  dfm() %>% 
  dfm_lookup(data_dictionary_LSD2015[1:2]) %>% 
  dfm_group(party)

print(brexit_sent)
```

## Plotting sentiment analysis

To plot our sentiment analysis, we use the subsetted dataframe from MPDataset and we join it with our sent_df to get the name of the parties.

Here you can see two plots. The first illustrates frequencies of positive and negative words targeted to the word "Brexit" for each UK party. 

In the second plot, we converted frequencies to a new measure score ("sent_score") where, using the natural logarithm, parties with more positive words relative to negative ones get a sent_score<0 and parties with more positive words relative to negative ones get a sent_score>0. Parties with a sent_score=0 are the ones which do not have the word 'Brexit' in their electoral programmes, thus the dictionary did not label them. 

We added the option to drop non labelled parties at the beginnig for better visualization purposes. 
```{r}
sent_df <- brexit_sent %>% 
  convert(to = "data.frame") %>% 
  #subset(!(doc_id %in% c("51330", "51420", "51621"))) %>% #remove parties with no sentiment assignment
  rename(party = doc_id) 

# joining the sentiment dataframe with the dataframe containing the parties names for each party code
# transforming the joining variable into the same class
MPD_uk_parties$party <-  as.character(MPD_uk_parties$party)

# leftjoining the datasets 
uk_joined <- left_join(sent_df, MPD_uk_parties, by = "party", copy = TRUE )

# frequency plot
uk_joined %>%
  pivot_longer(c(positive, negative)) %>% 
  ggplot(aes(x = value, y = partyname , fill= as.factor(name)))+
  geom_bar(position="dodge", stat="identity", width= 0.5) +
  labs(title = "Sentiment frequency plot for 'Brexit'", y = "UK Parties", x = "Frequency", fill = "sentiment") +
  theme_minimal()

# manually create the score
uk_joined$sent_score <- log((sent_df$positive + 0.5) / (sent_df$negative + 0.5))

# sentiment plot
uk_joined %>% 
  pivot_longer(c(positive, negative)) %>% 
  ggplot(aes(x = reorder(partyname, sent_score), y = sent_score)) + 
  geom_point(size = 3, colour = "skyblue") +
  coord_flip() + 
  labs(x = "UK Parties", y = "Estimated sentiment") +
  scale_y_continuous(expand = c(0.1, 0.1), breaks = c( -1, 0, 1), labels = c("Negative", "Neutral", "Positive")) +
  theme_bw()
```


# Credits

https://github.com/quanteda

Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A (2018). “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software, 3(30), 774. doi: 10.21105/joss.00774, https://quanteda.io.

Young, L and S Soroka. (2012). “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication 29(2): 205-231.

Burst, Tobias / Krause, Werner / Lehmann, Pola / Lewandowski, Jirka / Matthieß, Theres / Merz, Nicolas / Regel, Sven / Zehnter, Lisa (2021): Manifesto Corpus. Version: 2021-1. Berlin: WZB Berlin Social Science Center.

Pennings, Paul / Keman, Hans, Vrije Universiteit Amsterdam, Comparative Electronic Manifestos Project in cooperation with the Social Science Research Centre Berlin (Andrea Volkens, Hans-Dieter Klingemann), the Zentralarchiv für empirische Sozialforschung (GESIS), and the Manifesto Research Group (chairman: Ian Budge) (2006)

Volkens, Andrea / Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2021): The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR). Version 2021a. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpds.2021a

Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthieß Theres / Merz, Nicolas / Regel, Sven / Weßels, Bernhard / Zehnter, Lisa (2020): The Manifesto Data Collection: South America. Version 2020b. Berlin: Wissenschaftszentrum Berlin für Sozialforschung (WZB). https://doi.org/10.25522/manifesto.mpdssa.2020b
---
title: "Quanteda Package"
subtitle: "Sentiment analysis using the Manifesto Corpus"
author: "Laura Menicacci, Dinah Rabe"
date: "04/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to our tutorial!

In the following minutes we will go through the basics of the **Quanteda** package and make sense of how quantitative text analysis work in R. 

- what you'll find in the following tutorial 
- how we built it "we tried to add as much useful information as possible but here we will only show sentiment analysis with the ManifestoR database", why we chose to do sentiment analysis ???
- we will use the ManifestoR library to show some functionalities and how to work with text data

Let's start!

## Basics

Here are the main packages you'll need to install, plus some recommended ones that can help to extend or complete some functionalities (e.g. readtext takes files and returns them in a type of data.frame that can be used directly with the corpus() constructor function). 

```{r cars}
library(tidyverse)
library(quanteda)

library(quanteda.textmodels) #explain why there are three other packages - sister packages to complete quanteda tools
library(quanteda.textplots)
library(quanteda.textstats)
library(quanteda.sentiment)

library(readtext) #readtext package to read in different types of text data


```

# Data Importing

To import data we make use of the **readtext** package. 

## Pre-formatted files

Pre-formatted files are mainly imported in a “spreadsheet format”. **path_data** is the location of sample files that we will use.

```{r}
path_data <- system.file("extdata/", package = "readtext")
```

Most of the times, the pre-formatted file is stored with one column containing the text and additional columns storing document-level variables. If this is the case, then we can use read.csv() to import.

```{r}
dat_inaug <- read.csv(paste0(path_data, "/csv/inaugCorpus.csv"))
```

Alternatively, it is possible to import character values (comma- or tab-separated). readtext is able to import and read any file containing text and any associated document-level variable, as in this example. 

```{r pressure, echo=FALSE}
dat_dail <- readtext(paste0(path_data, "/tsv/dailsample.tsv"), text_field = "speech")
```

# Multiple text files

We can also load multiple text files at once that are stored in the same folder or subfolders. Individual text files usually do not contain document-level variables, but we can create them.

In this example, the directory /txt/UDHR contains text files (".txt”) of the Universal Declaration of Human Rights in 13 languages.

You can generate document-level variables based on the file names using the **docvarnames** and **docvars** from argument. 

- **dvsep = "_"** specifies the value separator in the filenames. 
- **encoding = "ISO-8859-1"** determines character encodings of the texts.

```{r}
dat_udhr <- readtext(paste0(path_data, "/txt/UDHR/*"))

dat_eu <- readtext(paste0(path_data, "/txt/EU_manifestos/*.txt"),
                    docvarsfrom = "filenames", 
                    docvarnames = c("unit", "context", "year", "language", "party"),
                    dvsep = "_", 
                    encoding = "ISO-8859-1")
str(dat_eu)

#If you are using Windows, you need might need to specify the encoding of the file by adding encoding = "utf-8". In this case, imported texts might appear like <U+4E16><U+754C><U+4EBA><U+6743> but they indicate that Unicode characters are imported correctly.
```

These are just some of the multiple ways to load data with quanteda. For furher information (e.g. you may want to know how to upload text with different encodings) take a look at https://tutorials.quanteda.io/import-data/. 

TO INCLUDE: how to work with Corpus, how to work with Tokens, how to work with DFM and some statistical analyses

#Working with ManifestoR

load manifestoR package: for this package we need an API key and some specific functions to check availability of data.
WHAT IS MANIFESTOR? and what is manifesto corpus? 
"The Manifesto Corpus is a digital text collection of electoral programs based on the collection and coding that was conducted for the generation of the Manifesto Project Main dataset.
The Manifesto Corpus contains three types of informations: machine-readable texts, meta-information for each document (such as language and title), and (for some documents) annotations/codes on the quasi-sentence level.
----> EXPLAIN CMP CODES!

we will use data from the latest elections in united kingdom to make a sentiment analysis
```{r}
#library(tm)
#library(NLP)

library(manifestoR)

mp_setapikey(key.file = "manifesto_apikey.txt")

available_uk<- mp_corpus(countryname == "United Kingdom") #function to download docs from manifesto project, we chose last manifestos in UK 
 
available_uk #71 docs
names(available_uk)

manifesto_corpus <- available_uk %>%
  as.data.frame(with.meta = TRUE) %>%
  corpus(docid_field = "manifesto_id", unique_docnames = FALSE) %>% 
  #docvars(field = "cmp_code") %>% 
  corpus() ## quanteda's corpus function: Creates a corpus object from available sources

manifesto_corpus

manifesto_corpus %>%
  docvars() %>% 
  names() #meta data information


```

#Subsetting the Corpus

We will do our sentiment analysis only with some parties in the English parliament, so we choose only two parties. We will take the Scottish National Party. 


```{r}
scotland_corpus <- manifesto_corpus %>%
  corpus_subset(party == "51902")

```

#Tokenization and Document-Feature-Matrix

Tokenization is particularly important for pre-processing and cleaning the texts. One can easily remove nubmers, punctuation or stopwords. Moreover, it is simple to transform all text to lower case or stem words.

```{r}
tokenized_manifesto <- scotland_corpus %>%
  tokens() %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english")) %>%
  tokens_wordstem()

```

#Constructing a document-feature-matrix with dfm

-function dfm()
- drop all quasi-sentences with headline codes (“H”), uncoded (“0”,“000”) and with codes missing (NA). 
- combine all quasi-sentences coded with the same code to one document
- Term frequencies can be transformed using the dfm_weight function. Here, we use it to calculate the proportion of words per document (scheme = "prop").
- then subset the dfm for two features of four specific codes: 108: european union positive, 110: european union negative, 301: federalism


```{r}
manifesto_dfm <- tokenized_manifesto %>% 
  dfm() %>% 
  dfm_subset(!(cmp_code %in% c("H", "", "0", "000", NA))) %>% 
  dfm_group(cmp_code) %>% 
  dfm_weight(scheme = "prop") %>% 
  dfm_subset(cmp_code %in% c("301", "110", "108")) 

```


#Plotting frequencies

```{r}
feature_frequencies_cat <- manifesto_dfm %>% textstat_frequency(n = 10, group = cmp_code)

feature_frequencies_cat %>%
  mutate(cmp_code = factor(group, labels = c("European Union: Positive", "Federalism", "European Union: Negative"))) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = cmp_code)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "share of words per category") +
  facet_wrap(~cmp_code, ncol = 2, scales = "free") +
  coord_flip()

```

#Keyword in context search 

Quanteda also provides a nice way to view text passages based on certain key words. The kwic (for keyword in context) allows you to use for a text string or pattern.

```{r}

scotland_corpus %>%
  tokens() %>%
  kwic(phrase("brexit"), window = 10) %>%
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))


```

#Targeted sentiment analysis

Quanteda facilitates dictionary based searchs. The following example illustrates how to conduct a targeted sentiment analysis. We use the corpus created above based on Scotland party and tokenize it into words. We then keep only tokens that include the word “President” as well as the ten words before and after every occurence of “Brexit”.

Quanteda has integrated a sentiment dictionary constructed by Young & Soroka (2012) stored in data_dictionary_LSD2015. The dictionary contains thousands of positive and negative words or word stems.


```{r}
brexit_tokens <- scotland_corpus %>%
  tokens()
  tokens_select("Brexit", selection = "keep", valuetype = "glob", case_insensitive = TRUE, window = 10, padding = FALSE, verbose = TRUE)

brexit_dfm <- brexit_tokens %>%
  dfm() %>% 
  dfm_lookup(data_dictionary_LSD2015) 

brexit_dfm %>% 
  dfm_group(date)

```


Quanteda (and its “sister”-packages quanteda.textstats, quanteda.textplots, quanteda.textmodels) has many more functions. In particular the textstat_* functions of quanteda.textstats are powerful and can well applied to manifestos.

#other visualizations??  

```{r}

```


###CREDITS 

https://tutorials.quanteda.io
https://manifesto-project.wzb.eu/information/documents/corpus
https://github.com/quanteda

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
